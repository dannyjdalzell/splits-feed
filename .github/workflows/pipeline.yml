name: pipeline

on:
  workflow_dispatch:
  schedule:
    # every 15 minutes
    - cron: "*/15 * * * *"

jobs:
  pipeline:
    runs-on: ubuntu-24.04
    env:
      PYTHONUNBUFFERED: "1"

    steps:
      # ---------- Checkout ----------
      - name: Checkout
        uses: actions/checkout@v4

      # ---------- Python ----------
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      # ---------- OCR (images → staged) ----------
      - name: OCR splits (images → staged)
        run: |
          mkdir -p audit_out
          python scripts/splits_ocr.py \
            --images images \
            --out audit_out/splits_staged.csv

      # ---------- NEW: Sheets → Twitter (always fresh to workspace) ----------
      - name: Sheets → Twitter ingest (always-fresh Export → workspace)
        run: |
          set -euo pipefail
          mkdir -p sources/sheets/twitter
          curl -Lsf "https://docs.google.com/spreadsheets/d/e/2PACX-1vT39ngJbPzNRjcnKVG-Oehiy4qzyrghIvCI0FQbaBj2jc9LYGLbMUZaCQDGN8Ck_8Q465hqsR4AYz3k/pub?gid=77061416&single=true&output=csv" \
            > sources/sheets/twitter/tweets.csv
          echo "[sheets] wrote $(wc -l < sources/sheets/twitter/tweets.csv) rows → sources/sheets/twitter/tweets.csv"

      # ---------- Twitter text analysis ----------
      - name: Twitter text analysis (signals)
        run: |
          mkdir -p audit_out
          python scripts/analyze_twitter_text.py \
            --csv sources/sheets/twitter/tweets.csv \
            --dict dictionaries \
            --out audit_out/twitter_text_signals.csv

      # ---------- Normalize + merge ----------
      - name: Normalize + merge sources
        run: |
          mkdir -p audit_out
          python scripts/normalize_and_merge.py \
            --splits audit_out/splits_staged.csv \
            --twitter audit_out/twitter_text_signals.csv \
            --out audit_out/boardroom_inputs.csv

      # ---------- Promote staged → splits.csv (fallback boardroom_inputs) ----------
      - name: Promote staged → splits.csv (with fallback to boardroom_inputs)
        run: |
          python - <<'PY'
          import os, sys, pandas as pd
          ROOT = os.getcwd()
          AOUT = os.path.join(ROOT,"audit_out")
          staged = os.path.join(AOUT,"splits_staged.csv")
          board  = os.path.join(AOUT,"boardroom_inputs.csv")
          OUT    = os.path.join(ROOT,"splits.csv")
          def exists_nonempty(p): return os.path.exists(p) and os.path.getsize(p)>0
          def promote_from_df(df):
              need={"away_team","home_team"}
              if not need.issubset(df.columns): return None
              df = df[(df["away_team"].astype(str).str.len()>1)&(df["home_team"].astype(str).str.len()>1)]
              if df.empty: return None
              cols = ["timestamp","league","away_team","home_team","market","tickets_pct","handle_pct","line","source"]
              miss = [c for c in cols if c not in df.columns]
              for c in miss: df[c]=""
              return df[cols]
          if exists_nonempty(staged):
              df=pd.read_csv(staged,dtype=str).fillna("")
              out=promote_from_df(df)
              if out is not None:
                  out.to_csv(OUT,index=False); print("PROMOTE_OK (staged) rows:",len(out)); sys.exit(0)
              print("staged present but no valid rows; falling back")
          if exists_nonempty(board):
              df=pd.read_csv(board,dtype=str).fillna("")
              if {"team_a","team_b"}.issubset(df.columns):
                  df=df.rename(columns={"team_a":"away_team","team_b":"home_team"})
              out=promote_from_df(df)
              if out is not None:
                  out.to_csv(OUT,index=False); print("PROMOTE_OK (fallback) rows:",len(out)); sys.exit(0)
          print("nothing promotable (no valid rows)")
          PY

      # ---------- Live delta analysis ----------
      - name: Live delta analysis (rolling; stop 15m pre-start)
        run: |
          python scripts/live_delta_analysis.py

      # ---------- Commit outputs (safe rebase) ----------
      - name: Commit outputs
        run: |
          git config user.name  "splits-bot"
          git config user.email "actions@users.noreply.github.com"
          git add sources/sheets/twitter/tweets.csv || true
          git add audit_out/twitter_text_signals.csv audit_out/boardroom_inputs.csv || true
          git add splits.csv reports || true
          git diff --cached --quiet && { echo "no changes"; exit 0; }
          git fetch origin
          git pull --rebase origin main || git rebase --strategy-option=theirs origin/main
          git commit -m "ci: refresh + promote + live-delta (auto)"
          git push

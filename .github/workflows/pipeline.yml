name: pipeline

on:
  workflow_dispatch:
  schedule:
    - cron: "*/15 * * * *"   # every 15 minutes
  push:
    branches: [ main ]
    paths:
      - "scripts/**"
      - "dictionaries/**"
      - ".github/workflows/pipeline.yml"

permissions:
  contents: write

concurrency:
  group: pipeline-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHONUNBUFFERED: "1"

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      - name: Ensure dirs
        run: |
          mkdir -p ./sources/sheets/twitter
          mkdir -p ./audit_out

      - name: Fetch Google Sheet (tweets.csv)
        env:
          SHEETS_CSV_URL: ${{ secrets.SHEETS_CSV_URL }}
        run: |
          if [ -z "${SHEETS_CSV_URL}" ]; then
            echo "[WARN] SHEETS_CSV_URL secret not set; leaving previous tweets.csv if present."
          else
            curl -fsSL "$SHEETS_CSV_URL" -o ./sources/sheets/twitter/tweets.csv
            echo "downloaded tweets.csv"
          fi
          test -f ./sources/sheets/twitter/tweets.csv && wc -l ./sources/sheets/twitter/tweets.csv || echo "tweets.csv missing (continuing)"

      - name: Analyze Twitter text (graded)
        run: |
          if [ -f ./sources/sheets/twitter/tweets.csv ]; then
            python scripts/analyze_twitter_text.py \
              --csv  ./sources/sheets/twitter/tweets.csv \
              --dict ./dictionaries \
              --out  ./audit_out/twitter_text_signals.csv
          else
            echo "no tweets.csv; skipping text analysis"
            : > ./audit_out/twitter_text_signals.csv
          fi

      - name: Normalize & Merge (build staged)
        run: |
          python scripts/normalize_and_merge.py
          test -f ./audit_out/splits_staged.csv && wc -l ./audit_out/splits_staged.csv || true
          test -f ./audit_out/boardroom_inputs.csv && wc -l ./audit_out/boardroom_inputs.csv || true

      - name: Promote (dictionary-validated; staged → fallback)
        run: |
          python - <<'PY'
          import os, sys, json, re
          import pandas as pd

          def exists_nonempty(p): return os.path.exists(p) and os.path.getsize(p) > 0

          # ---- load dictionaries ----
          DICT_DIR = "dictionaries"
          league_files = {
              "NFL":   os.path.join(DICT_DIR, "nfl.json"),
              "MLB":   os.path.join(DICT_DIR, "mlb.json"),
              "NBA":   os.path.join(DICT_DIR, "nba.json"),
              "NHL":   os.path.join(DICT_DIR, "nhl.json"),
              "NCAAF": os.path.join(DICT_DIR, "ncaaf_fbs_seed.json"),
          }
          TEAM2LEAGUE, NORM = {}, {}
          for lg, fp in league_files.items():
              if not os.path.exists(fp): continue
              data = json.load(open(fp))
              teams = data.get("Teams") or data.get("teams") or []
              for t in teams:
                  name = (t.get("name") or t.get("Name") or "").strip()
                  if not name: continue
                  k = name.lower()
                  TEAM2LEAGUE[k] = lg
                  NORM[k] = name
                  for al in t.get("aliases", []) + t.get("Aliases", []):
                      a = al.strip().lower()
                      if a:
                          TEAM2LEAGUE[a] = lg
                          NORM[a] = name

          def norm_team(x:str):
              x = (x or "").strip()
              if not x: return None, None
              key = re.sub(r"[^a-z0-9 .'-]", " ", x.lower())
              key = re.sub(r"\s+", " ", key).strip()
              if key in NORM:
                  nm = NORM[key]
                  return nm, TEAM2LEAGUE.get(key)
              if key in TEAM2LEAGUE:
                  return x, TEAM2LEAGUE[key]
              return None, None

          def promote_from_df(df: pd.DataFrame):
              if not {"home_team","away_team"}.issubset(df.columns): return None
              rows = []
              for _, r in df.iterrows():
                  ht, at = str(r.get("home_team","")).strip(), str(r.get("away_team","")).strip()
                  if len(ht) < 2 or len(at) < 2: continue
                  n_ht, lg_ht = norm_team(ht)
                  n_at, lg_at = norm_team(at)
                  if not n_ht or not n_at: continue
                  league = lg_ht if lg_ht == lg_at else (lg_ht or lg_at or "")
                  if not league or league == "MIXED": continue
                  rows.append({
                      "timestamp": str(r.get("timestamp","")),
                      "league": league,
                      "away_team": n_at,
                      "home_team": n_ht,   # **bottom = home**
                      "market": str(r.get("market","UNKNOWN")) or "UNKNOWN",
                      "tickets_pct": str(r.get("tickets_pct","")),
                      "handle_pct":  str(r.get("handle_pct","")),
                      "line":        str(r.get("line","")),
                      "source":      str(r.get("source","pipeline")),
                  })
              if not rows: return None
              out = pd.DataFrame(rows).drop_duplicates(subset=["league","away_team","home_team","market","line"])
              return out

          # 1) Prefer staged (from OCR + resolver)
          staged = "audit_out/splits_staged.csv"
          if exists_nonempty(staged):
              df = pd.read_csv(staged, dtype=str).fillna("")
              out = promote_from_df(df)
              if out is not None and len(out):
                  out.to_csv("splits.csv", index=False)
                  print("PROMOTE_OK (staged) rows:", len(out)); sys.exit(0)
              print("staged present but no dictionary-valid rows; falling back")

          # 2) Fallback to boardroom inputs (normalized aggregator)
          fallback = "audit_out/boardroom_inputs.csv"
          if exists_nonempty(fallback):
              dfb = pd.read_csv(fallback, dtype=str).fillna("")
              if {"team_a","team_b"}.issubset(dfb.columns):
                  dfb = dfb.rename(columns={"team_a":"away_team","team_b":"home_team"})
              elif not {"away_team","home_team"}.issubset(dfb.columns):
                  print("fallback lacks expected columns:", dfb.columns.tolist()); sys.exit(0)
              dfb["market"] = dfb.get("market","UNKNOWN").replace("", "UNKNOWN")
              dfb["source"] = dfb.get("source","pipeline_fallback")
              out = promote_from_df(dfb)
              if out is not None and len(out):
                  out.to_csv("splits.csv", index=False)
                  print("PROMOTE_OK (fallback) rows:", len(out)); sys.exit(0)

          print("nothing promotable (no dictionary-valid rows)")
          PY

      - name: Commit outputs
        run: |
          git config user.name  "splits-bot"
          git config user.email "actions@users.noreply.github.com"
          git add sources/sheets/twitter/tweets.csv || true
          git add audit_out/twitter_text_signals.csv audit_out/boardroom_inputs.csv || true
          git add splits.csv || true
          git commit -m "ci: refresh + promote (tweets, staged → splits.csv)" || echo "no changes"
          git push
